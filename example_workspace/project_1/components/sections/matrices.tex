Now that we have a basic handle on vectors, let us focus on matrices. In this short section, we shall touch upon several uses of matrices within the context of deep learning. One use is to organize stored data as a table. Another use that we touch on very lightly is for describing covariances of variables. A third use, which is the most important for us is linear transformations, and this is the role that the weight matrices (parameters), $W$ in Model~II, and $W^{[\ell]}$ in Model~III play. As with vectors, our exposition is only the tip of the ice-berg and for a more expanded introduction we recommend \cite{boyd2018introduction} as a first reading.

A matrix is a list of numbers organized in rows and columns. For example this is the {\em matrix} $X$ with $4$ rows and $3$ columns,
%
\begin{equation}
\label{eq:matrix-example-1}
X
=
\begin{bmatrix}
    0.4 & -1 & 4\\
    1.2 & 0 & -0.5 \\
    0 &  2.1 & 3 \\
    5 & 2.1 & -10
 \end{bmatrix}.
\end{equation}
%
We say that this is a $4 \times 3$ matrix and we can refer to each element as $x_{i,j}$ where $i=1,2,3,4$ denotes the index of the row and $j=1,2,3$ is for the column. It is common to use capital letters for matrices and then refer to individual elements with the lower case letters. For example $x_{3,2} = 2.1$. One very basic use for matrices is to describe tabular data, similarly to how we described data in  Section~\ref{sec:summation}. To match the description there we should notice that $x^{(i)}_j = x_{i,j}$ (the observation or sample with index $i$, denoted via the superscript $(i)$ in $x^{(i)}_j$ is the row, and the feature $j$, denoted via the subscript $j$ in $x^{(i)}_j$ is the column). We denote the set of all matrices with $m$ rows and $n$ columns as ${\mathbb R}^{m \times n}$. Note that in the case of a data matrix matching the data table in \eqref{eq:data-table} we need a matrix in ${\mathbb R}^{n \times p}$ because it has $n$ rows (observations), and $p$ columns (features).

If the number of rows and the number of columns is equal, we say that the matrix is {\em square}. The set of square matrices of dimension $n$ is denoted as ${\mathbb R}^{n \times n}$. In addition to being square, if all the elements $x_{i,j}$ where $i \neq j$ are $0$, then we say the matrix is {\em diagonal} (it only has non-zero entries on the diagonal which is all entries $x_{i,i}$ for $i=1,\ldots,n$). One very important type of diagonal matrix is the $n \times n$ {\em identity matrix}, denoted $I$, which has $0$ values everywhere except on the diagonal where it has $1$ values. For example this is the $3 \times 3$ identity matrix,
%
\begin{equation}
\label{eq:identity}
I
=
\begin{bmatrix}
    1 & 0 & 0\\
    0 & 1 & 0 \\
    0 & 0 & 1 \\
 \end{bmatrix}.
\end{equation}
%
Vectors can be viewed as special cases of matrices. For example consider the column vector, $u$ and the row vector $v$,
\begin{equation}
\label{eq:vec-as-matrix}
u = 
\begin{bmatrix}
2 \\
0 \\
6 \\
\end{bmatrix},
\qquad \qquad
v = 
\begin{bmatrix}
2 & 0 & 6\\
\end{bmatrix}.
\end{equation}
%
Viewed as matrices we can say that $u$ is a $3 \times 1$ matrix, and $v$ is a $1 \times 3$ matrix (namely $u \in {\mathbb R}^{3 \times 1}$ and $v \in {\mathbb R}^{1 \times 3}$). We can also recall the transpose operator, $\top$, for vectors that converts a column vector to a row vector with the same numbers, and vice-versa. Here, for the example values we have for $u$ and $v$ in \eqref{eq:vec-as-matrix}, we obviously see that $u^\top = v$ and $v^\top = u$.

Indeed for any matrix of dimension $m \times n$, if we transpose the matrix we get a matrix of dimension $n \times m$ where the $(i,j)$-th entry (row $i$ and column $j$) of the transposed matrix, is the $(j,i)$-th entry of the original matrix. For example, the transpose of the $4 \times 3$ matrix $X$ from \eqref{eq:matrix-example-1} is the $3 \times 4$ matrix given by
%
\begin{equation}
\label{eq:matrix-example-1-t}
X^\top =
\begin{bmatrix}
    0.4 & 1.2 & 0 & 5 \\
    -1 & 0 & 2.1 & 2.1 \\
    4 & -0.5 & 3 & -10
\end{bmatrix}.
\end{equation}
%

When a matrix is square, applying the transpose to it does not change the dimensions. Incidentally a square matrix $A$ such that $A = A^\top$ is called a {\em symmetric matrix}, because any entry $a_{i,j}$ on one side of the diagonal, equals the matching entry $a_{j,i}$ on the other side of the diagonal. In statistics, data science, and some aspects of deep learning, an important place where symmetric matrices arise is with variance and covariance descriptions of features under study. In particular the {\em covariance matrix} (sometimes called the {\em variance-covariance matrix}), is often denoted as $\Sigma$ (not to be confused with the summation notation reviewed in Section~\ref{sec:summation}), and it captures the variances and covariances of the features under study. In particular the $(i,j)$-th element of $\Sigma$ is the covariance between feature $i$ and feature $j$, and this equals the $(j,i)$-th element as well. For the case where $i=j$, i.e., on the diagonal, this element is the variance of feature $i$. We do not make use of such matrices further in this text, but refer the reader to an elementary exposition such as in chapter~3 of \cite{nazarathy2020statistics}, or the references there-in.

One of the most important things that we can do with matrices is {\em matrix multiplication}. For simplicity let us first take two square matrices, each in ${\mathbb R}^{3 \times 3}$.
%
\begin{equation}
\label{eq:matrix-a-b-for-mult}
A = 
\begin{bmatrix}
2 & 0 &3 \\
0 & 1 & 1 \\
2 & 0 & 0
\end{bmatrix}
\qquad
\text{and}
\qquad
B = 
\begin{bmatrix}
4 & 1 &0 \\
1 & 0 & 0 \\
0 & 0 & 3
\end{bmatrix}.
\end{equation}
%
Now, in this case, the product $C= AB$ is a new $3 \times 3$ matrix, where the entry $c_{i,j}$ is the inner product of the $i$-th row of $A$ and the $j$-th column of $B$. For example at $i=2$ and $j=3$ we have,
\[
c_{2,3} = a_{2,1} b_{1,3} + a_{2,2} b_{2,3} + a_{2,3} b_{3,3} = 0\cdot 0 ~+~ 1\cdot 0 ~+~ 1\cdot 3 = 3.
\]
%
In the same manner, to get all other $8$ elements of the matrix $C,$ we can do all other inner products. As the reader can verify, the matrix $C$ turns out to be,
%
\begin{equation}
\label{eq:matrix-example}
C = A B = \begin{bmatrix}
8 & 2 & 9 \\
1 & 0 & 3 \\
8 & 2 & 0 \\
\end{bmatrix}.
\end{equation}
%

Note that multiplication of scalars is commutative since for two scalars (numbers), $a$ and $b$, we always have that $ab = ba$. With matrices this is not the case. For example $\tilde{C}=BA$ yields a different result to $C= AB$. As the reader may verify,
%
\begin{equation}
\label{eq:matrix-example-2}
\tilde{C} = BA = 
\begin{bmatrix}
8 & 1 & 13 \\
2 & 0 & 3 \\
6 & 0 & 0 \\
\end{bmatrix}
\neq C.
\end{equation}
%

Up to now we multiplied square matrices of the same dimension, but we can also, in certain cases, multiply non-square matrices. The rules defined for matrix multiplication dictate that it cannot be done for any two matrices, but only in certain cases. In particular take $A \in {\mathbb R}^{m \times n}$ and $B \in {\mathbb R}^{n \times r}$, then $A$ has $n$ columns and $B$ has the same number, $n$, of rows. This means that rows of $A$ and columns of $B$ are of the same dimension, $n$, and thus we can compute inner products between rows of $A$ and columns of $B$. Otherwise, if these dimensions do not match, then matrix multiplication is not defined. So for example we can compute the product of a $4 \times 3$ matrix by a $3 \times 7$ matrix, but we cannot compute the product of a $4 \times 3$ matrix and a $4 \times 7$ matrix. Note that sometimes, depending on the dimension, we can compute a product $AB$, but not the product $BA$, or vise versa.

We also mention that the identity matrix, such as the $3 \times 3$ example shown in \eqref{eq:identity} is special in terms of multiplication. As the reader can verify, if we multiply either $A$ or $B$ from \eqref{eq:matrix-a-b-for-mult} by $I$ (from either side), then the result does not change. That is, $AI = A$, $IA = A$, $BI = B$, and $IB = B$. This holds for any dimension of the identity matrix and any other matrix, where the identity matrix and the other matrix can be multiplied (with matching dimensions). Hence~$I$ in matrices acts like $1$ in scalars (for any scalar $a \in {\mathbb R}$, $1a = a$ and $a1 = a$).

For our purposes, an important form of matrix multiplication is when the second matrix is actually a vector. In this case we call this {\em matrix-vector} multiplication. In particular take $W \in {\mathbb R}^{K \times p}$ and take a column vector $x \in {\mathbb R}^{p \times 1}$ (we could have just stated that $x$ is an element of ${\mathbb R}^p$, but for purposes of matrix multiplication, we consider it as a matrix). This notation matches the left side of \eqref{eq:softeq} of Model~II, where we multiply a $K \times p$ parameter matrix $W$ by the input vector $x$. 

According to the rules of matrix multiplication, the multiplication $Wx$ yields a $K \times 1$ matrix as a result, or simply a $K$ dimensional vector. For example, here is a schematic of this matrix-vector multiplication with $K = 5$ and $p=3$,
%
\begin{equation}
\label{eq:matrix-vector-mult} 
\underbrace{
\begin{bmatrix}
w_{1,1} & w_{1,2} & w_{1,3}  \\
w_{2,1} & w_{2,2} & w_{2,3}\\
w_{3,1} & w_{3,2} & w_{3,3}\\
w_{4,1} & w_{4,2} & w_{4,3}\\
w_{5,1} & w_{5,2} & w_{5,3}\\
\end{bmatrix}
}
_
{
\textrm{Parameter matrix $W \in {\mathbb R}^{5 \times 3}$}
}
~~
\underbrace{
\begin{bmatrix}
x_1  \\
x_2 \\
x_3 \\
\end{bmatrix}
}
_
{\textrm{Input vector $x \in {\mathbb R}^3$}}
~
=
~
\underbrace{
\begin{bmatrix}
w_{1,1}x_1 + w_{1,2} x_2 + w_{1,3} x_3 \\
w_{2,1}x_1 + w_{2,2} x_2 + w_{2,3} x_3\\
w_{3,1}x_1 + w_{3,2} x_2 + w_{3,3} x_3\\
w_{4,1}x_1 + w_{4,2} x_2 + w_{4,3} x_3\\
w_{5,1}x_1 + w_{5,2} x_2 + w_{5,3} x_3\\
\end{bmatrix}
}
_{
\textrm{Output vector in ${\mathbb R}^5$}
}.
\end{equation}

%
As is evident, each entry of the output vector is the inner product between the associated row of $W$ and the input vector $x$.

We should also note that in \eqref{eq:softeq} of Model~II there is the bias vector $b$ added to $Wx$. This is an addition of two vectors in ${\mathbb R}^K$. Thus we have the vector $z = b + Wx$, represented as follows (for an example with $p=3$ and $K=5$),
%
\begin{equation}
\label{eq:model-ii-vector-add} 
z = 
\begin{bmatrix}
b_1 \\
b_2 \\
b_3 \\
b_4 \\
b_5 \\
\end{bmatrix}
+
\begin{bmatrix}
w_{1,1}x_1 + w_{1,2} x_2 + w_{1,3} x_3 \\
w_{2,1}x_1 + w_{2,2} x_2 + w_{2,3} x_3\\
w_{3,1}x_1 + w_{3,2} x_2 + w_{3,3} x_3\\
w_{4,1}x_1 + w_{4,2} x_2 + w_{4,3} x_3\\
w_{5,1}x_1 + w_{5,2} x_2 + w_{5,3} x_3\\
\end{bmatrix}.
\end{equation}
%
This representation of $z$ in \eqref{eq:model-ii-vector-add}, exactly agrees with \eqref{eq:small-zk-log-mult} which appeared earlier, before reviewing vector and matrix operations. Note that \eqref{eq:dense-layer} for Model~III defining the action of a layer, $S^{[\ell]}(b^{[\ell]} + W^{[\ell]} u)$ can now also be understood as a similar operation to \eqref{eq:matrix-vector-mult} and \eqref{eq:model-ii-vector-add}. We provide further details in Section~\ref{sec:putting-bits-together}.
