We should mention that we have only skimmed vector and matrix operations, but our presentation was enough for understanding the representations of models I -- III.  
In further investigation one may consider many more useful aspects of vectors and matrices in texts such as \cite{boyd2018introduction}, or slightly more advanced linear algebra in \cite{strang2019linear}, or references there-in.

We also mention that an important part of matrix notation that often appears is the {\em matrix inverse}.  
We do not cover this concept fully here, but only hint at its meaning.  
One way to motivate it is to note that there is not an operation for division of matrices, and instead there is the operation of multiplication by an inverse.  
With two scalar numbers $a \in {\mathbb R}$ and $b \in {\mathbb R}$ with $b \neq 0$, one way to express the division $a/b$ is via the multiplication $a \frac{1}{b}$ or $a \, b^{-1}$ (i.e., multiply $a$ by the inverse of $b$).  
With matrices, in certain cases, for a matrix $B$ we have a matrix called the {\em inverse} and denoted as $B^{-1}$.  
Then (when dimensions agree) we may consider multiplications such as $A B^{-1}$ for some matrix $A$.  
Matrix inverses often appear when one considers systems of linear equations, and in the context of deep learning this may be when considering elementary linear models.  
See for example chapter~2 of \cite{LiquetMokaNazarathy2024DeepLearning} for an example of where such concepts are needed.  
In deep learning models such as our models I -- III here, one does not often encounter matrix inverses.

We note that sometimes data samples may not be in vector form.  
In that case, we convert them to vectors before inputting to the model.  
For example, if the input is a black and white (monochrome image) then it can be in the form of a matrix.  
Still we typically {\em vectorize} it, to convert the matrix to a vector so that it can be used as an input to models I -- III.  
Such vectorization can simply be done by taking an $n \times m$ matrix and spreading out the columns (or rows) to create a long vector of dimension $m \cdot n$.  
In general, vectorization is an important operation in deep learning.  
To see an example, return to the matrix $X$ in \eqref{eq:matrix-example-1}.  
A vectorized form of $X$ taken {\em column wise} is $x = (0.4,\, 1.2,\, 0,\, 5.0,\, -1,\, 0,\, 2.1,\, 2.1,\, 4,\, -0.5,\, 3,\, -10)$.  
An alternative is to vectorize {\em row wise} where we obtain $x = (0.4,\, -1,\, 4,\, 1.2,\, 0,\, -0.5,\, 0,\, 2.1,\, 3,\, 5,\, 2.1,\, -10)$.

We also mention that many matrix and vector operations can be conducted {\em element-wise}.  
For example taking two vectors $u \in {\mathbb R}^n$ and $v \in {\mathbb R}^n$, we have already seen that $u+v$ is an element-wise operation giving a vector where the $i$-th entry is $u_i+v_i$.  
An element-wise product of $u$ and $v$ can also be defined similarly, where the $i$-th entry of the element-wise product is $u_i v_i$.  
This type of product is sometimes denoted as $u \odot v$ and is called the {\em Hadamard product}.  
With the exception of addition, element-wise operations are not extremely common in general statistics and mathematics, but in deep learning they occur more often.  
One could have also for example had element-wise division.  
Importantly, when we have some function $f: {\mathbb R} \to {\mathbb R}$ (scalar inputs and scalar outputs), we can sometimes apply it element-wise to a whole vector or matrix.  
For example if the input vector is $u = (u_1, u_2, u_3)$, we can agree to apply $f(~)$ element-wise on $u$ and write the result as $f(u) = (f(u_1), f(u_2), f(u_3))$.  
This is very common with activation functions in internal layers of deep neural networks such as Model~III.  
More details are in in the sequel.  
Element-wise operations can be applied to matrices just like they are to vectors.

We finally mention the concept of a {\em tensor}.  
As the reader may have gauged, there is a progression from scalars, to vectors, and then from vectors to matrices.  
Each time another ``dimension'' also known as ``rank'' or ``order'' is added.  
Scalars do not need indexing; vectors require a single index; and matrices require two indices.  
The next level up is a $3$-tensor which can be viewed as a layering of multiple matrices of the same dimension on top of each other.  
That is, in addition to rows, and columns, there is also a depth component.  
A very typical application is a red, green, and blue image which can be comprised of three matrices, one for the values of the red pixels, one for the green, and one for the blue.  
One can even consider higher dimensional tensors, yet in many deep learning models, $3$-tensors suffice.  
For example in convolutional neural networks, $3$-tensors are used throughout.