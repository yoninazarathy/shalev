Now we are ready to revisit Model~III and fill in a few of the missing details. With deep neural networks, like Model~III, the versatility of the model allows us to in principle approximate any function. In particular, say that in reality we have some unknown function $f^*: \mathbb{R}^p \longrightarrow {\mathbb R}^q$, which is only available to us via data points $(x^{(i)}, y^{(i)})$ with $x^{(i)} \in {\mathbb R}^p$, $y^{(i)} \in {\mathbb R}^q$,  and $y^{(i)} = f^*(x^{(i)})$. If it is a binary classification scenario then $q=1$ and the output can be considered as an element in $[0,1]$. If it is a multi-class classification scenario then $q=K$ and the output can be considered as an element in ${\mathbb R}^K$. If it is a regression problem then again $q=1$ and the output can be any scalar value in ${\mathbb R}$. Finally, in other applications we have vector output ${\mathbb R}^q$ for some $q>1$. In all of these cases, with Model~III we can try and approximate~$f^*(~)$  via the model function \eqref{eq:model-III-f}.

Models I and II are shallow neural networks as they only involve a single layer. As such, they are not able to approximate any arbitrary function very well. However, Model~III when used with multiple layers, is a very rich model, and in principle no matter what $f^*(~)$ we have in reality, with enough training data, and enough computation power for training (gradient descent), we can obtain,
%
\begin{equation}
\label{eq:mod3-approx}
f_{(b^{[1]}, W^{[1]}), \ldots, (b^{[L]}, W^{[L]})} (x) \approx f^*(x).
\end{equation}
%
That is, in general, there exist some parameters ${(b^{[1]}, W^{[1]}), \ldots, (b^{[L]}, W^{[L]})}$ that will enable our model to approximate any function $f^*(~)$. See chapter~5 of \cite{LiquetMokaNazarathy2024DeepLearning} for further discussion about the expressivity of deep neural networks. Let us now dive into a few more details of Model~III.

Recall from  \eqref{eq:dense-layer} that every layer $\ell$ of the model is of the form $f^{[\ell]}(u) = S^{[\ell]}(b^{[\ell]} + W^{[\ell]} u)$ and the model consists of a composition of such layers via \eqref{eq:generalRecursiveModel}. We have already encountered an affine operation similar to $b^{[\ell]} + W^{[\ell]} u$ in the context of Model~II where it was $b + Wx$. Like Model~II we sometimes use a softmax for $S^{[\ell]}(~)$ in the last layer, $\ell=L$, especially when our goal is multi-class classification. Yet for inner layers, $\ell = 1,\ldots,L-1$, we use a (scalar) activation function $\sigma: \mathbb R \to \mathbb R$ and then the structure of the function $S^{[\ell]}(~)$ is via element-wise activations $S^{[\ell]}(z) = \big(\sigma\left(z_{1}\right) ~ \ldots ~\sigma\left(z_{N}\right)\big)$, where $z = (z_1, \ldots, z_N)$. In general, $\sigma(~)$ can be a sigmoid function, as defined in \eqref{eq:first-shallow-view}, or it can be one of other common activation functions in deep learning such as {\em ReLU} or {\em Tanh}; see chapter~5 of \cite{LiquetMokaNazarathy2024DeepLearning} for details. Note that in many texts one just writes, $\sigma(b^{[\ell]} + W^{[\ell]} u)$ for the layer, implying that $\sigma(~)$ is applied element-wise.

We can now revisit Figure~\ref{FFNN} which illustrates a small version of such a model with $L=4$. Observe that for this network,  the input dimension is $p=4$ and the output dimension is $q=1$. The function of each layer $f^{[\ell]}(u) = S^{[\ell]}(b^{[\ell]} + W^{[\ell]} u)$ operates on the outputs of the previous layer (or the input to the network in case $\ell=1$) and yields {\em activation values} (sometimes just called neuron values) of layer $\ell$. We denote these values as $a^{[\ell]}$ and thus for $\ell=1,\ldots,L$,
%
\begin{equation}
\label{eq:s-deep-step}
a^{[\ell]} = f^{[\ell]}(a^{[\ell-1]}),
\qquad
\textrm{where}
\quad
a^{[0]} = x,
\quad
\text{and}
\quad
\hat{y} = a^{[L]}.
\end{equation}
%

What remains to be specified are the dimensions of each of the activations in the network. For example, as the reader may inspect based on the number of neurons (blue nodes) per layer in Figure~\ref{FFNN}, we have that,
%
\begin{equation}
\label{eq:nn-example-dims}
x = a^{[0]} \in {\mathbb R}^4,
\qquad
a^{[1]} \in {\mathbb R}^4,
\qquad
a^{[2]} \in {\mathbb R}^3,
\qquad
a^{[3]} \in {\mathbb R}^5,
\qquad
\hat{y} = a^{[4]} \in {\mathbb R}.
\end{equation}
%
These particular dimensions are part of the specific architecture of the deep neural network and they also imply the dimensions of the model parameters. For example consider layer $\ell=3$ with $f^{[3]}(u) = S^{[3]}(b^{[3]} + W^{[3]} u)$. From \eqref{eq:s-deep-step} we see that $f^{[3]}(~)$ maps $a^{[2]}$ to $a^{[3]}$ and further via \eqref{eq:nn-example-dims} we see that this is a mapping from ${\mathbb R}^3$ to ${\mathbb R}^5$. If we revisit the way that matrix-vector multiplication works as in \eqref{eq:matrix-vector-mult}, we see that we must have $W^{[3]} \in {\mathbb R^{5 \times 3}}$. Further from \eqref{eq:model-ii-vector-add}, we must have $b^{[3]} \in {\mathbb R}^5$. In a similar manner we can consider other layers and see that the dimensions of the individual parameters, $(b^{[1]}, W^{[1]}), \ldots, (b^{[L]}, W^{[L]})$, are specified by the number of activations. The reader may also appreciate that the number of arrows going from layer~2 to layer~3 in Figure~\ref{FFNN} is $15$, exactly matching the $5\times 3=15$ entries of $W^{[\ell]}$. Hence each element in the weight matrix $W^{[\ell]}$ can be considered as an individual {\em weight} on the arrow connecting a neuron from layer $2$ to layer $3$.

As a final illustration, let us unpack the function of the model for the example network from Figure~\ref{FFNN}. When considering the operation at each of the layers one after the other we obtain,

\begin{equation}
\label{eqn:opened-out-example-2}
\hat{y} =S^{[4]}(W^{[4]}S^{[3]}(W^{[3]}S^{[2]}(W^{[2]}S^{[1]}(W^{[1]}x+b^{[1]})+b^{[2]}) +b^{[3]})+b^{[4]}).
\end{equation}
In fact, the reader may work out that the number of parameters is,
%
\begin{equation}
\label{eq:61-is-the-number}
d = \underbrace{4\times4 + 4}_{\text{Hidden layer 1}} + \underbrace{3\times 4 + 3}_{\text{Hidden layer 2}}+ \underbrace{5\times 3 + 5}_{\text{Hidden layer 3}}  + \underbrace{1 \times 5 +1}_{\text{Output layer}}  = 61.
\end{equation}
%
Hence, when training such a model via gradient descent, each time we compute the gradient, we obtain a gradient vector in ${\mathbb R}^{61}$. More realistic models may have many more activations (neurons), and sometimes more layers as well. Hence the number of parameters, $d$, easily climbs to millions or more, and this is why training large deep neural networks may be very expensive in terms of time, hardware, and power.
