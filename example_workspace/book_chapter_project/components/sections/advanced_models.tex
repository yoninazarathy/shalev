Deep neural networks have been extended way beyond Model~III, 
yet the good news is that for more advanced models, generally one does not necessarily need more advanced mathematics. 
In this section, we present some key mathematical constructs used beyond models I--III. 
We present the {\em convolution} operation used in {\em convolutional neural networks} (CNNs), 
{\em word embedding} used in sequence models such as {\em recurrent neural network} (RNNs) or {\em long short term memory} (LSTM) models, 
and the {\em attention mechanism} used in {\em transformer} models that power current state-of-the-art {\em large language models}.

Convolutional neural networks have revolutionized the field of computer vision. 
Fundamentally, CNNs leverage the concept of the convolution, 
a mathematical operation that involves applying a {\em filter}, also known as a {\em kernel}, across an input data grid to extract local patterns or features. 
This process is akin to sliding a window over an image and with each move computing the {\em inner product} 
between the elements of the (vectorized) kernel and the elements of the corresponding region of the (vectorized) image. 
The convolution operation between two matrices $W$ and $x$ is denoted as $z = W \star x$. 
There are different versions of the convolution operation. 
Here, we present a basic version and for more general scenarios, refer to chapter 6 of \cite{LiquetMokaNazarathy2024DeepLearning}. 
To understand this basic version, assume that $x$ is a grayscale image of size $100 \times 100$ and the kernel $W$ is a $3\times 3$ matrix of weight parameters represented as
%
\begin{equation}
\label{eq:conv-kernel}
W = 
\begin{bmatrix}
    w_{1,1} & w_{1,2} & w_{1,3} \\
    w_{2,1} & w_{2,2} & w_{2,3} \\
    w_{3,1} & w_{3,2} & w_{3,3} \\
\end{bmatrix}.
\end{equation}
%
Note that the kernel matrix $W$ is usually much smaller than the image $x$. 
We slide the kernel over the image to perform the convolution operation. 
As in the case of Model~III, the best values for the weight parameters ($w_{i, j}$ entries in $W$) are learned during the training process.

In the basic version for this example, the result $z$ of the convolution operation is a matrix of dimension $(100 - 3 + 1) \times (100 - 3 + 1) = 98 \times 98$ 
with the $(i, j)$-th element of $z$ computed as 
%
\begin{equation}
\label{eq:conv-explicit}
z_{i,j} = \sum_{i'=0}^{2} \sum_{j'=0}^{2} x_{i+i', j+j'}  \cdot \, w_{i'+1, j'+1}
\qquad
\textrm{for}
\qquad i, j \in \{1, \dots, 98\}.
\end{equation}
%
Here \( x_{i+i', j+j'} \) represents the pixel value at position \( (i+i', j+j') \) in the input image, 
and \( w_{i'+1, j'+1} \) represents the weight parameter at position \( (i'+1, j'+1) \) in the kernel. 
For instance, to compute the value $z_{1, 1}$,% on the output feature map $z$, 
we perform the calculation,
%
\begin{equation}
\label{eq:very-spcific-conv}
z_{1,1} = \sum_{i'=0}^{2} \sum_{j'=0}^{2} x_{1+i', 1+j'} \cdot w_{i'+1, j'+1}.
\end{equation}
%
Note that $z_{1,1}$ is the result of an inner product between the vectorized $W$ and the vectorized $3\times 3$ top-left submatrix of $x$. 

Similarly, we compute all the elements of $z$ by sliding the kernel over the image and performing such an inner product each time between the kernel $W$ and the corresponding submatrix of $x$. 
After performing the convolution operation between the image and the kernel, a convolutional layer in a CNN applies an activation function, just like in Model~III. 
This yields what is sometimes called a {\em feature map} that highlights certain patterns or features present in the image. 
Now just like in Model~III, the feature map serves as input to subsequent layers in the CNN for further processing and analysis.

It's important to note that CNNs involve other operations, such as pooling, 
and various architectural configurations including multiple channels (feature maps) per layer, skip connections, 
integration of fully connected layers, and others. 
See chapter~6 of \cite{LiquetMokaNazarathy2024DeepLearning} for more details. 
We also note that from a historical perspective, the work in \cite{krizhevsky2012imagenetxxxqqq} was pivotal in highlighting the strength of deep learning, and convolutional neural networks in particular.

While CNNs are excellent for tasks like image recognition, 
sequential data such as text, often requires a different approach. 
This is where sequence models like recurrent neural networks (RNNs) and long short term memory (LSTM) models come into play. 
Unlike CNNs which process the entire input at once, 
RNNs and LSTMs process the data sequentially one element at a time. 
In doing so, these models maintain an internal state that captures information about previous elements in the sequence.

One key challenge in using RNNs and LSTMs for natural language processing tasks is how to represent words as numerical vectors such that these models can understand the data. 
This is where {\em word embedding} becomes useful. 
Word embedding is a technique used to represent words as vectors, where the vectors corresponding to similar words, remain close to each other. 
Via the vector representation of the words, similarity between any two words is measured by the cosine of the angle between the corresponding two vectors using formula~\eqref{eq:cosine-angle-vec}. 

For example, the word \texttt{king} could be represented by the vector $(0.41, \, 1.2,\, 3.4, \,-1.3)$ and the word \texttt{queen} can be represented by a relatively similar vector such as $(0.39, \, 1.1, \, 3.5, \, 1.6)$. 
Then a completely different word such as \texttt{mean} might be represented by a vector such as $(-0.2,\, -3.2,\, 1.3,\, 0.8)$. 
One can now verify in this example, that the cosine of the angle between the words \texttt{king} and \texttt{queen} is about $0.729$ while the cosine of the angle between \texttt{mean} and the other two words is lower, which is at about $-0.04$ for \texttt{king} and $0.156$ for \texttt{queen}, respectively. 

With such a word embedding approach, the typical way we process input text is to convert each word (or sometimes a similar notion known as a {\em token}) to a vector. 
Hence the input to a model, is not just a single vector $x$ as is the case for models I--III, but is rather a sequence of vectors. 
Then an RNN model or LSTM model processes this sequence, one by one, keeping an internal state and also resulting in an output sequence. 
This technique has proven valuable for many language tasks including translation among others. 
For a description of how classical models such as RNN models or LSTM models deal with such data, see chapter~7 of \cite{LiquetMokaNazarathy2024DeepLearning}.

Recurrent neural networks, long short term memory models, 
and a few variants were the main sequence models in deep learning up to recent times. 
However, in the last few years, following the 2017 paper \cite{vaswani2017attention}, 
a completely different approach, called {\em transformer} models has emerged and is now the main tool used in contemporary large language models. 
Transformers overcome, limitations of RNNs and LSTM models, such as difficulty in parallelization and difficulty in capturing long-range dependencies 
(even though LSTM models are explicitly designed for enabling long range memory). 
Transformers address these limitations, primarily by leveraging on an idea called the {\em attention mechanism}. 
Unlike RNNs and LSTMs, which process inputs sequentially, transformers process all words simultaneously, enabling highly efficient parallel computation. 
This parallelization makes transformers particularly well-suited for handling long sequences, such as those encountered in machine translation, 
document summarization tasks, and general interactions with large language models via chat. 
While we leave a complete description of the transformers architecture to chapter~7 of \cite{LiquetMokaNazarathy2024DeepLearning}, or other sources, 
let us see now how basics of the attention mechanism can be described via inner products and the softmax function.

When processing each word, 
the attention mechanism allows the model to focus on relevant parts of the input sequence. 
That is, by “attending” from each word to every other word, we capture dependencies across the entire sequence. 
Imagine reading a long piece of text and trying to summarize it. 
Instead of reading the entire text from start to finish every time, 
one can generate a summary by focusing on the most relevant parts, or key words. 
This selective focus is analogous to what the attention mechanism does. 
At a high level, we assign a weight to each input word based on its relevance to the current word being processed. 
This weight determines how much attention the model should pay to that input word when generating the output associated with the current word.

Mathematically, to understand the attention mechanism, 
consider a sequence of words $x^{{\langle 1 \rangle}},\ldots, x^{{\langle T \rangle}}$ where
each $x^{{\langle t \rangle}}$ is a vector representing a word (or token) using our word embedding scheme. 
Our goal is to compute the attention weights for a specific current word, $x^{{\langle t \rangle}}$. %, in the sequence. 
We begin by calculating a {\em score} (also called {\em alignment score}) for all other input words, $x^{{\langle j \rangle}}$, based on their similarity to $x^{{\langle t \rangle}}$. 
A basic form for such an alignment score is using the inner product,
%
\begin{equation}
\label{eq:score}
s(x^{{\langle t \rangle}},x^{{\langle j \rangle}})=(x^{{\langle t \rangle}})^\top x^{{\langle j \rangle}}.% \qquad 
%j = 1,\ldots,T.
\end{equation}

These scores, computed for $j = 1,\ldots,T$, are then passed through the softmax function, $S_{\text{softmax}}(~)$, defined in \eqref{eq:softeq}, 
which squashes them into a probability vector $(\alpha_1, \ldots, \alpha_T)$. 
Namely the attention weight of any input word $j$ from the perspective of the current word $t$ is,
%
\begin{equation}
\label{eq:attentionweight}
\alpha_{j} = \frac{e^{s(x^{{\langle t \rangle}},x^{{\langle j \rangle}})}}{\sum_{t'=1}^T e^{s(x^{{\langle t \rangle}},x^{{\langle t' \rangle}})}}.
\end{equation}
%

The probability vector $(\alpha_1, \ldots, \alpha_T)$ represents how much attention each input word $x^{{\langle j \rangle}}$ 
should receive when we handle the current word $x^{{\langle t \rangle}}$. 
Intuitively, due to the inner product operation used in \eqref{eq:score}, 
input words that are more similar to the current word will have higher attention weights, 
indicating that they are more relevant for generating the output. 
Conversely, input words that are less relevant will have lower attention weights, 
meaning they contribute less to the output generation process. 
By selectively attending to the most relevant parts of the input sequence, 
the attention mechanism enables us to capture long-range dependencies and learn complex patterns in the data. 
This yields a setup that is highly effective for a wide range of natural language processing tasks, 
from language translation to text generation.