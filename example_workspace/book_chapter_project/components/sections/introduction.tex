By now, deep learning has become an indispensable tool in science, permeating fields such as astronomy, genomics, climate science, robotics, materials science, and medical science.
Its transformative impact extends beyond traditional boundaries, finding applications not only in theoretical domains but also in practical, real-world scenarios such as deciphering the intricacies of the human genome, predicting climate patterns, and enhancing surgical precision in neurosurgery.
Deep learning serves as the linchpin for advancements in data analysis, image recognition, natural language processing, and decision-making systems, catalyzing breakthroughs that were once deemed unattainable.
The past two decades have seen great advances in deep learning.
See \cite{lecun2015deep} for a brief overview.

There are multiple ways in which one can try and understand the workings of deep learning.
One popular way to describe deep learning is via a hands-on programming approach as in \cite{howard2020deep}.
This is good for someone directly implementing deep learning with a specific programming language such as Python.
However, if one only seeks to understand what is going on in the realm of deep learning, the use of programming can be too specific and time-consuming.
Hence, a more reasonable approach is to understand underlying basic ideas, often using mathematical notation.
This is the approach that we adopted in our recent book, {\em Mathematical Engineering of Deep Learning}, \cite{LiquetMokaNazarathy2024DeepLearning}.
Other similar texts that also require mathematical notation include {\em Understanding Deep Learning} \cite{prince2023understanding} and the more classic {\em Deep Learning} \cite{goodfellow2016deep}, among others.

In all these texts, mathematical notation is very effective at pinpointing ideas, in a dense and concise manner.
The problem however is that many scientists using deep learning are not always comfortable with such notation.
Understanding mathematical notation requires prerequisite knowledge.
For example, for our book, we believe that having taken 3 to 4 university-level mathematics courses is necessary for a thorough appreciation of the deep learning concepts that we present.
Naturally, one cannot obtain such knowledge and experience overnight.
Nevertheless, there is a spectrum between in-depth understanding to avoidance, and we believe that through a short piece such as this work here, the reader can get a basic understanding of the meaning of the notation, and as a consequence gain an entry point to deep learning as well.
Hence this work here can be treated as a quick guide for mathematics notation in the context of deep learning.

To visit our goal of starting to understand mathematical notation, we are motivated by three key models from the world of deep learning and machine learning, which we aptly call Model~I, Model~II, and Model~III.
Model~I is the sigmoid model, also known as the logistic model.
Model~II is the softmax model, also known as the multinomial regression model.
Model~III is the general fully connected feedforward deep neural network, also known as the feedforward model, and sometimes called a multi-layer perceptron.

Each of these models operates on some input which we denote as $x$ and creates an output which we denote as $\hat{y}$.
The input $x$ can be a series of numbers, an image, a voice signal, or essentially any structured input.
The output $\hat{y}$ is a probability between $0$ and $1$ for Model~I.
It is a list (or vector) of such probabilities, summing up to $1$ for Model~II.
And in the case of Model~III, it can be either a probability, a vector of probabilities, or any other type of output that our application dictates.

In these cases, models are presented with an input feature vector $x$ and return an output $\hat{y}$.
With the sigmoid model, the output is a probability value indicating the likelihood of a binary event, serving as a foundational element in binary classification tasks.
The softmax model, on the other hand, extends this concept to multiple classes, assigning probabilities to each class for multi-class classification problems.
In the case of the general fully connected neural network, its flexibility allows for intricate mappings between inputs and outputs, enabling the network to learn complex relationships within the data.
Understanding these fundamental models is a crucial step towards unraveling the mathematical framework that underpins deep learning.

Deep learning is the area where models are created (training) and then used (production/inference) for solving machine learning or artificial intelligence tasks.
The types of models and tasks are too numerous to cover in this presentation.
Instead, let us focus on a simple supervised learning task, encompassing both binary and multi-class classification.
In a {\em binary classification} scenario, a deep learning model is presented with inputs and aims to determine a binary outcome.
For instance, in a medical diagnosis context, the model might assess whether an input image $x$ is associated with a particular pathology or not.
This may be presented in the form of a probability $\hat{y}$.

Similarly, in multi-class classification, the model, still operating on such an input image $x$, may determine which class from a finite collection best represents the input.
In a medical image classification scenario, particularly in the realm of diagnosing brain diseases, the deep learning model is presented with an input brain scan and tasked with classifying it into one of several possible conditions.
For instance, the model may need to distinguish between various types of brain tumors, such as glioblastoma, meningioma, and pituitary adenoma.
The output of the model is typically a list (or vector) of probabilities associated with each of these classes (brain tumor types).
The class with the highest probability is typically chosen as the model's prediction.

The remainder of this document is structured as follows.
In Section~\ref{sec:motivating-deep-learning-models}, we see a mathematical description of models I, II, and III.
Our aim with the early introduction of the models is to motivate the mathematical sections that follow where we unpack the notation presented for these models.
Hence a reader reading Section~\ref{sec:motivating-deep-learning-models} should not be intimidated by the notation, but rather try to embrace it as it is unpacked and described in the sections that follow.
In Section~\ref{sec:summation} we review summation notation through the application of data standardization (mean and variance computation).
Most scientists would have seen and used such summation notation before, hence this section is mostly a review.
In Section~\ref{sec:sets-and-functions} we present an elementary view of sets, and functions.
This is in no way an exhaustive description of set theory, but rather aims to place the reader in the mindset of set notation, and especially the "in" symbol, $\in$, that is often used, as well as the notation ${\mathbb R}$ for the set of real numbers and the declaration of functions using the arrow, $\to$, from the input set to the output set.
In Section~\ref{sec:vectors} we outline the notion of vectors, which in greatest simplicity can be viewed as lists of numbers.
Ideas such as inner products and norms are also introduced.
In Section~\ref{sec:matrices} we discuss matrices and in particular matrix-vector multiplication.
This operation is central in deep learning.
We then comment on additional aspects of vectors and matrices in Section~\ref{sec:further-notes}.
In Section~\ref{sec:gradient} we overview the concept of gradients, as these are the objects used when training deep learning models.
We also see a basic version of the gradient descent algorithm.
Then in Section~\ref{sec:putting-bits-together} we put a few of the mathematical pieces together, shining more light on fully connected feedforward deep neural networks.
In Section~\ref{sec:more-advanced-models} we briefly discuss mathematical ideas used in a few extension models, including convolutional neural networks, recurrent neural networks, and the attention mechanism which is used in many contemporary large language models.
We finally conclude in Section~\ref{sec:conclusion}.