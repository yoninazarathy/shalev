Almost all of mathematics starts with the notion of a {\em set}. Formally, a set is an unordered collection of unique items, and sets are typically denoted like $\{7,\, 2.5,\, \texttt{hello}\}$, where the curly braces, $\{\}$, indicate that this is a set. In this particular example the set is composed of the number $7$, the number $2.5$, and the text \texttt{hello}. Like anything in mathematics, we can name sets, and we could have also written ${\cal A} = \{7,\, 2.5,\, \texttt{hello}\}$ to name this set as  ${\cal A}$.

Each of $7$, $2.5$, and \texttt{hello} are {\em elements} of the set ${\cal A}$. And we can write $7 \in {\cal A}$, to imply that $7$ is an element of ${\cal A}$, and similarly with $2.5$ and \texttt{hello}. This ``in'' symbol, $\in$, is useful for statements such as: ``for all $u \in {\cal A}$ do something with $u$''. This means, ``do something with $7$, and do something with $2.5$, and do something with \texttt{hello}''. We can also say that $4 \notin {\cal A}$, because $4$ is not an element of ${\cal A}$. That is $\not\in$ is ``not in'' with the slash across the $\in$ symbol. 

Sometimes we define sets in less explicit ways. For example ${\cal B} = \{0,2,4,6,\ldots,20\}$ is the set of all even numbers between $0$ and $20$ including $0$ and $20$. And this is clear to us even though we did not write out every element of ${\cal B}$. We can also use sets within summation notation. For example, 
%
\begin{equation}
%\label{eq:4-summation-set}
\sum_{u \in {\cal B}} \sqrt{u},
\end{equation}
%
implies summing up all of the square roots of the elements of the set ${\cal B}$ (as the reader may verify, the result is approximately $31.77$). Note  that our previous way of using summation notation can also be written in terms of sets. For example, the summation in \eqref{eq:4-summation} can be written as,
%
\begin{equation}
%\label{eq:4-summation-set}
\sum_{i \in \{1,2,3,4\}} h^{(i)},
\end{equation}
%
as this shows that the variable $i$ runs on each element of the set $\{1,2,3,4\}$. 

One very important set is the set of real numbers, denoted ${\mathbb R}$. Unlike the example sets ${\cal A}$, ${\cal B}$, or $\{1,2,3,4\}$ which only have a finite number of elements, the set ${\mathbb R}$ has every number on the real number line and is hence not a finite set. We again can speak about elements of ${\mathbb R}$. So for example it is true that $7 \in {\mathbb R}$,  and it is also true that $\texttt{hello} \not\in {\mathbb R}$. When we consider the parameters of Model~I in \eqref{eq:first-shallow-view}, we can write $b_0 \in {\mathbb R}$ to signify that $b_0$ is a real number.

We can also denote the set of real numbers as $(-\infty, \infty)$ implying that it is the interval of all numbers that are greater than $-\infty$ and less than $\infty$, and this means all real numbers. There are other sets that we can denote in a similar way, for example $[-1,1]$ is the set of all numbers greater than or equal to $-1$ and less than or equal to $1$. Another option is the set $(0,\infty)$ which means the set of all positive numbers (greater than $0$). The set $[0,1]$ is the set of all real number between $0$ and $1$ inclusive, and this is basically all numbers that can describe a probability. A related set $(0,1)$ contains all the numbers between $0$ and $1$ but without the boundaries $0$ and $1$.

One can study and discuss sets much further, and in a more formal manner, or even in very formal means that relate to mathematical logic. But our purpose here is simply to introduce minimal notation. For this, we present a few more sets when dealing with vectors and matrices in the sections below. But first, at this point, with our basic understanding of sets, we are ready to discuss the notion of a {\em function}. 

Put simply, a function is an operation that transforms elements from one set to elements of another set. We can for example denote our function as $f$ and think that it operates on inputs from the set ${\cal A} = \{7,\, 2.5,\, \texttt{hello}\}$ and gives us outputs from the set of real numbers~${\mathbb R}$. Formally this can be denoted as,
%
\begin{equation}
\label{eq:some-func}
f: {\cal A} \to {\mathbb R},
\end{equation}
%
and this notation tells us that all of the possible inputs to the function $f(~)$ are the elements of~${\cal A}$. It further says that the outputs must be elements of the real numbers ${\mathbb R}$. This declaration of the function via \eqref{eq:some-func} says that for every $u \in A$ we should have an answer of what the function gives, and we denote this as $f(u)$. Note that we sometimes write $f(\cdot)$ to indicate the function where ``$\cdot$'' just stands for the place where the argument $u$ should appear. 

A declaration such as \eqref{eq:some-func} does not define how the function works. To do so, we must be more explicit either with a formula, or an algorithm, or a lookup table. In this example, since the input set has a small number of heterogeneous elements, let us specify the function via a lookup table approach. In particular we can state, $f(7) = 3.4$, $f(2.5) = -2.1$, and $f(\texttt{hello}) = 20.3$. This would then specify exactly what the function $f(~)$ does for every $u \in {\cal A}$. 

In other cases, we can specify how the function works with a formula. This is often common for functions $f: {\mathbb R} \to {\mathbb R}$. An arbitrary example of such a function from ${\mathbb R}$ to ${\mathbb R}$ is,
%
\begin{equation}
\label{eq:arb-func-r-to-r}
f(u) = 3\cos(e^{u-2}).
\end{equation}
%
The reader would have seen plots previously where such a function, or others, are plotted where on the horizontal axis we plot $u$ and on the vertical axis we plot $3\cos(e^{u-2})$. Importantly, for every $u \in {\mathbb R}$ we have a specific $y = f(u)$, and the plot is a connection of the points $(u,f(u))$ on the plane, essentially for every $u \in {\mathbb R}$ (or realistically on some smaller set which defines the domain of the plot). Also note that in this case the function \eqref{eq:arb-func-r-to-r} is {\em composed} of other operations and functions such as the cosine function and the exponentiation function. Indeed function composition is a very common operation where outputs of one function are used as inputs of another. An example is with Model~III as in \eqref{eq:generalRecursiveModel} where we have $L-1$ function compositions and each function represents the operation of a layer of neural network.

In deep learning we use functions in multiple ways. One way is for constructing models such as I -- III. Another way is for specifying the whole model as a function. A third way is for construction of loss functions. Let us now highlight such uses.

First in terms of construction of models, as we can see for Model~I in \eqref{eq:first-shallow-view},  we define the {\em sigmoid function} 
%
\begin{equation}
\label{eq:sig-as-func}
\sigma_{\text{Sig}}: {\mathbb R} \to [0,1].
\end{equation}
%
This function takes any real valued scalar as input, denoted as $z$ in \eqref{eq:first-shallow-view}, and the output is a number in the range $[0,1]$. Note that for this sigmoid function we have a formula, $1/(1+e^{-z})$, which exactly describes how to compute $\sigma_{\text{Sig}}(z)$. A schematic plot of this function is inside the right circle in Figure~\ref{fig:niceneuron}.


Further, for Model~II in \eqref{eq:softeq} we have the softmax function, $S_{\textrm{Softmax}}(z)$ as a building block. This function can be declared as,
%
\begin{equation}
\label{eq:softmax-as-func}
S_{\textrm{Softmax}}: {\mathbb R}^K \to {\mathbb R}^K,
\end{equation}
%
since it takes $K$-dimensional vectors as inputs and returns $K$-dimensional vectors as outputs. We describe it further in the next section after we discuss vectors. Similarly, for Model~III, we have activation functions for deep feedforward neural networks, $S^{[\ell]}(\cdot)$. We discuss such activation functions in Section~\ref{sec:putting-bits-together}. 

Now a different use of functions for the neural network models I, II, and III is that the whole model is a function that converts some input~$x$ to an output~$\hat{y}$. In terms of all three models, I -- III, the input $x$ is a $p$-dimensional list of numbers, or vector, a notion further discussed in the next section. For now, let us agree that this set is denoted as~${\mathbb R}^p$. The output is either a scalar value (an element of ${\mathbb R}$ or an element of $[0,1]$) or a vector which is an element of ${\mathbb R}^K$ (a vector or list of $K$ numbers). In particular, here are the functions described by these models.

For Model~I, yielding a probability output we can declare the function of the model as,
%
\begin{equation}
\label{eq:model-I-f}
f_{(b_0,w)}: {\mathbb R}^p \to [0,1].
\end{equation}
%
The representation in \eqref{eq:model-I-f} then tells us that inputs to the model are $x \in {\mathbb R}^p$ and outputs are going to be $\hat{y} \in [0,1]$. Now notice that we also decorate the function name $f$ with a subscript $(b_0,w)$, and this subscript signifies the parameters of the model. In particular, when we train a deep learning model such as this, we find suitable values of the parameters $b_0$ and $w$ for the data. These parameters then exactly dictate the specifics of our model function $f_{(b_0,w)}(~)$. The way the function actually works was already specified in \eqref{eq:first-shallow-view}. However, some specifics of that presentation, such as the inner product $w^\top x$, are explained in the next section. In Figure~\ref{fig:breast-log-curves} we see two example plots for two different instances of \eqref{eq:model-I-f} where each time we plot $\hat{y} = f_{(b_0,w)}(x)$. In (a) we see a case with $p=1$, and we also see data points for which the function was fit. In (b) we see a case with $p=2$, this time without the data points. 

\begin{figure}[h!] 
  \begin{subfigure}[b]{0.45\linewidth}
    \centering
    \includegraphics[width=0.95\linewidth, height=1.2\textwidth]{figures/Figure-LINK-UNIVARIATE-chapt.pdf} %trim=0 0 0 0, clip
    \caption{} 
%    \vspace{2ex}
  \end{subfigure}%% 
~
  \begin{subfigure}[b]{0.45\linewidth}
   % \centering
    \includegraphics[width=0.99\linewidth, angle=-5, trim={0 10 30 10}]{figures/logistic-2-features-cropped.pdf} %,trim=100 0 0 0, clip
    \caption{} 
%    \vspace{2ex}
  \end{subfigure} 
    \caption{Probability output using Model I. % prediction fitto the Wisconsin breast cancer dataset. 
    (a) A $p=1$ model with one feature. % \texttt{area\_mean} ($x_1$)and a confidence band. 
    (b) A $p=2$ model based on two features. %5\texttt{area\_mean} ($x_1$) and \texttt{texture\_mean} ($x_2$).
    }
    \label{fig:breast-log-curves}
\end{figure}


For Model~II, the function of the model can be specified as,
%
\begin{equation}
\label{eq:model-II-f}
f_{(b,W)}: {\mathbb R}^p \to {\mathbb R}^K,
\end{equation}
%
and in particular the output in this case is a $K$-dimensional vector. The parameters of the model are $(b,W)$, and more details about how the model operates are in the sequel.

For Model~III, we leave the specification of the type of output open. In some cases it can be a single probability value, so we may specify the output as a value in the set $[0,1]$. In other cases it can be a single real number, as one typically has in {\em regression problems}. Thus the output can be specified as a value in ${\mathbb R}$. We can also use Model~III for multi-class classification like Model~II and then the output is a vector as with Model~II, so the output is a value in ${\mathbb R}^K$ (also sometimes denoted ${\mathbb R}^q$). One way to write this is,
%
\begin{equation}
\label{eq:model-III-f}
f_{(b^{[1]}, W^{[1]}), \ldots, (b^{[L]}, W^{[L]})}: {\mathbb R}^p \to {\cal O},
\qquad
\text{where}
~{\cal O}~
\text{is}~[0,1],~\text{or}~
{\mathbb R},~\text{or}~{\mathbb R}^q.
\end{equation}
%
In all these scenarios of Model~III, the input is similar to the other two models, but for the output type there are several options. Observe also the rich set of parameters that the model has, namely, $(b^{[1]}, W^{[1]}), \ldots, (b^{[L]}, W^{[L]})$.

Another use of functions in deep learning is {\em loss functions}. In general when we train a model we are given a fixed dataset and wish to find the best set of parameters such that the model fits the data. For example with Model~II we would seek the best possible vector $b$ and matrix $W$ that we can find to match the data.  The way that this ``bestness'' is quantified is via a function that we artificially construct for the model. Unlike the model functions \eqref{eq:model-I-f}, \eqref{eq:model-II-f}, and \eqref{eq:model-III-f} which operate on the input $x$, the loss function is a function of the parameters, and the training data is fixed for this function and determines its shape.


\begin{figure}[h!] 
  \begin{subfigure}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=0.87\linewidth, trim=30 30 30 30]{figures/Non-convex-3d.png}
    \caption{} 
    \vspace{2ex}
  \end{subfigure}%% 
  \begin{subfigure}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=0.77\linewidth, trim=30 30 30 30]{figures/Non-convex-contour.pdf}
    \caption{} 
    \vspace{2ex}
  \end{subfigure} 
    \caption{An example plot of an hypothetical loss function (also known as loss landscape) when there are two parameters. (a) A 3D surface plot where we see that the function has multiple valleys (local minima). (b) The same function can be plotted using a contour plot, where each line along a contour maintains the same value of the loss function (like a topographical map).}
    \label{fig:GD-LR-SR}
\end{figure}

Now since each type of model has a different set of possible parameters, it is common to just call all the parameters $\theta$. In this case $\theta$ can signify $(b_0,w)$ for Model~I, $(b, W)$ for Model~II, and so fourth. We further call the set of all possible parameters $\Theta$ (and the specific form of this set depends on the model type that we use). The loss function, can then be written as,
%
\begin{equation}
C_{\textrm{Data}}: \Theta \to {\mathbb R},
\end{equation}
%
where the subscript, ``Data'', reminds us that the function's actual form depends on the training data that we have. Now in many cases, each $\theta \in \Theta$ contains many parameters (numbers). Especially for example with Model~III, where one can quickly get millions of parameters in a deep neural network.



The act of learning the parameters, or training the model, is the act of finding some $\theta \in \Theta$ where $C_{\textrm{Data}}(\theta)$ is as low as possible or close to the lowest value. This is an optimization problem where we try to minimize the loss $C_{\textrm{Data}}(~)$.

Now when the number of parameters in each $\theta$ is $1$ or $2$, it is possible to plot the loss function and then visualize the minimization of the loss. Such plots are typically not done for operational purposes but rather for pedagogic purposes. In particular, the case of $\theta$ being composed of two numbers $\theta_1$ and $\theta_2$ is easy to plot, and such plots allow us to also think about the techniques and challenges of minimizing loss in general. In Figure~\ref{fig:GD-LR-SR} we present a plot of such an hypothetical loss function.
