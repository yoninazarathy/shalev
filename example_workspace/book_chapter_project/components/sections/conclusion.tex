While our ultimate goal is deep learning, our focus in this document was mathematical background. 
For this we covered many elementary components of mathematical notation used in conveying deep learning ideas. 
Such a survey cannot replace solid mathematical foundations, yet we hope that it enables scientists that do not often use mathematics, to engage with mathematical descriptions of deep learning models and algorithms. 
We can also consider the presentation here as a gentle introduction (or review) for readers wanting to study our book, {\em Mathematical Engineering of Deep Learning} \cite{LiquetMokaNazarathy2024DeepLearning}, as well as the other books we mentioned at the introduction. 
While the presentation here does not empower the reader with all of the needed mathematical background, it does enable the reader to follow a solid portion of the content from such a book. 
Readers wishing to further strengthen their foundations are encouraged to follow standard courses and books in calculus, linear algebra, probability, and statistics.\footnotemark{}

\footnotetext{See \url{https://deeplearningmath.org/} for specific suggestions.}

This is a summary of the key mathematical content that we covered. 
We covered sets and their elements, as well as summation notation with sets. 
We covered basic set notation for the real numbers ${\mathbb R}$ as well as notation for vectors ${\mathbb R}^n$, and matrices ${\mathbb R}^{m \times n}$. 
We covered declaration of functions using notation such as $f: {\cal A} \to {\cal B}$, where ${\cal A}$ and ${\cal B}$ are sets. 
We covered basics of vectors, but without their geometric interpretation, a topic that the reader can pick up elsewhere. 
In particular we covered inner products, Euclidean norms, cosine of the angle, the Euclidean distance between vectors, and mean squared error. 
We also covered basics of matrices including elementary definitions, diagonal matrices, symmetric matrices, identity matrices, matrix transpose, and importantly matrix multiplication (including matrix-vector multiplication). 
We also mentioned element-wise operations on vectors and matrices, matrix inverses, and tensors. 
Finally we covered gradient vectors, the key component for gradient descent learning (optimization). 
As we saw, this dense manifest of ``basic mathematical knowledge'' can go a long way in helping to describe complex deep learning models, and we revisited our Models I -- III throughout, connecting the basic notation of these models to the elementary mathematical principles.

In terms of models beyond our example models I -- III, we briefly highlighted ideas of convolutions, word embedding, and the attention mechanism. 
Other aspects of deep learning that we did not cover include generative models such as generative adversarial networks, variational autoencoders, diffusion models, reinforcement learning, and graph neural networks. 
Admittedly, some of these concepts may require more mathematical background than we provided here. 
The reader may see chapter~8 of \cite{LiquetMokaNazarathy2024DeepLearning} for an overview of this assortment of topics. 