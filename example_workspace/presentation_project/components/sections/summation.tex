Raw data often requires \textit{preprocessing} before it can be used in a deep learning model. One such form of preprocessing is \textit{standardization of the data}, also sometimes called \textit{normalization of the data}. This involves subtraction of the mean and division by the standard deviation. Let us describe how this is done here and in the process review summation notation which is needed for other purposes also.

Assume that our data is $x^{(1)}, \ldots, x^{(n)}$ where each $x^{(i)}$ is some sample which is a list or vector of length $p$. We can also denote $x_j^{(i)}$ as the $j$-th element (or coordinate) of the $i$-th data sample. Hence we can think of the data as an $n$ by $p$ table, or matrix, where each row is a sample, and each column is a specific part of that sample, sometimes called a {\em feature}.

For instance, consider a dataset consisting of medical measurements for patients, where each patient's data includes their age, blood pressure, and cholesterol level. This means that $p=3$. As an example, this data may look as follows:
%
\[
\begin{array}{c}
% \hspace{4.4cm}
\text{Age} \quad \text{Blood Pressure} \quad \text{Cholesterol Level} \\
\end{array}
\]
\begin{equation}
\label{eq:data-table}
% \text{Raw Medical Data: }
\left[ \begin{array}{ccc}
50 & 120 & 200 \\
35 & 130 & 220 \\
\vdots & \vdots & \vdots \\
\vdots & \vdots & \vdots \\
\phantom{\text{Age}} & \phantom{\text{Blood Pressure}} & \phantom{\text{Cholesterol Level}}\\
\vdots & \vdots & \vdots \\
\vdots & \vdots & \vdots \\
45 & 125 & 190 \\
\end{array} \right]
\end{equation}
%
The number of observations $n$ is the number of rows in the table.
In this example, the second observation in the second row, $x^{(2)}= (35,130,220)$, corresponds to the information of the second patient. Similarly, $x_3^{(1)} = 200$ is the cholesterol level for the first patient. 

You can now take all the sample values for some feature $j$, and denote them as $x_j^{(1)}, \ldots, x_j^{(n)}$. This would be one column of the data.  
Now for this column we can compute the \textit{sample mean} and \textit{sample variance} respectively as,
%
\begin{equation}
\label{eq:stats-mean-var}
\overline{x}_j = \frac{1}{n} \sum_{i=1}^n x_j^{(i)},
\qquad
s^2_j = \frac{1}{n} \sum_{i=1}^n (x_j^{(i)} - \overline{x}_j)^2.
\end{equation}
%
Further, the \textit{sample standard deviation} is the square root of the sample variance and is denoted via $s_j$. Note that in statistics one sometimes divides by $n-1$ instead of $n$ for the sample variance, but this is a detail we will skip here. Our purpose in presenting \eqref{eq:stats-mean-var} is also that we review summation notation (with $\Sigma$, i.e., Sigma).

To review summation notation, consider an expression where we have some list of $4$ numbers, $h^{(1)}, h^{(2)}, h^{(3)}, h^{(4)}$ with $h^{(1)} = 2$, $h^{(2)} = 4$, $h^{(3)} = 0$, and $h^{(4)} = 10$. Then this arbitrary expression,
%
\begin{equation}
\label{eq:4-summation}
\sum_{i=1}^4 h^{(i)},
\end{equation}
%
is just shorthand for $h^{(1)} +h^{(2)} +h^{(3)} +h^{(4)}$. It thus equals $16$ in this example. One can think of the variable $i$ as ``running'' from $i=1$ all the way up to $i=4$. We could have also used $j$ instead of $i$ or any other variable name. One could have obviously had more complicated expressions with summation notation, such as for example,
%
\begin{equation}
\label{eq:4-summation-advc}
\sum_{i=1}^4 (h^{(i)} +h^{(5-i)} - 11)^2,
\end{equation}
%
which the reader can verify equals $100$.

Now in \eqref{eq:stats-mean-var} we first use summation notation in the most basic manner to compute the sample mean for feature $j$ which we denote as $\overline{x}_j$. This is then used to compute the sample variance for that feature which we denote as $s_j^2$. Then the sample standard deviation, $s_j$, is just the square root of $s_j^2$.

With our data, once we have the sample mean and sample standard deviation for each feature, we may standardize the data samples of each feature $j=1,\ldots,p$ and each observation $i=1,\ldots,n$ to obtain \textit{standardized samples},
%
\begin{equation}
\label{eq:ref-stand-z}
\tilde{x}^{(i)}_j = \frac{x^{(i)}_j - \overline{x}_j}{s_j}.
\end{equation}
%
These standardized observations can also be placed in an $n$ by $p$ table just like the original data. Now the standardized data for feature $j$, $\tilde{x}_j^{(1)}, \ldots, \tilde{x}_j^{(n)}$, has a sample mean of exactly~$0$ and a sample standard deviation of exactly~$1$. In the case the data samples of the feature are distributed according to a normal (Gaussian) distribution then most standardized samples would lie in the range $[-3,3]$. Even if the data is not normally distributed, the standardized samples will still lie in the vicinity of this range and are centered about $0$.

Such standardization is useful as it places the dynamic range of the model inputs on a uniform scale and thus improves the numerical stability of algorithms. It also allows us to use similar models for different datasets that may, without standardization, have completely different dynamic ranges. It is one of many tricks of the trade when dealing with data for deep learning. We also chose to present it here as a basic review of summation notation.
