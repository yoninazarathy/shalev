Now that we understand the basics of sets and functions, let us advance to the concept of a {\em vector}. As already seen in Models I -- III, the input $x$ to the model is a vector. This is a list of numbers which encodes some form of input data. Vectors can also appear as outputs. Specifically, in Model~II, and in some variants of Model~III, the output, $\hat{y}$ is also a vector. For example, with Model~II and $K=4$ we may have an output vector such as,
%
\begin{equation}
\label{eq:example-y-hat}
\hat{y} =
\begin{bmatrix}
0.14 \\
0.02\\
0.65 \\
0.19\\
\end{bmatrix},
\end{equation}
%
which in this case marks the probabilities of the four classes of classification (observe that in this example the numbers are non-negative and sum up to $1$). When we have a vector, we often relate to the individual components via a subscript, so that for example $\hat{y}_1 = 0.14$. In this example vector, we see that $\hat{y}_3 = 0.65$ is the highest probability, so if this is a classification output, then we would choose $\widehat{\cal Y} = 3$ as our classification choice using \eqref{eq:argmax-mp}.

The set of vectors of length or {\em dimension $n$} is denoted as ${\mathbb R}^n$. So for example, for the vector $\hat{y}$ of \eqref{eq:example-y-hat} we can write $\hat{y} \in {\mathbb R}^4$. Note that in different texts, vectors are denoted differently. For us, let us also write the same vector as $\hat{y} = (0.14,\, 0.02,\, 0.65,\, 0.19)$. The use of the round brackets, $(~)$, is sometimes associated with the term {\em tuple}, which is similar to a vector and for our purposes may mean the same thing. Also note that the way we wrote the vector in \eqref{eq:example-y-hat} is as a {\em column vector} (the vector is standing up). This way of writing is especially relevant when we do matrix-vector multiplication in the next section. Similarly, one may write $\hat{y} = [ 0.14~~ 0.02~~0.65~~0.19]^\top$, where in this case we first wrote a {\em row vector} $[ 0.14~~ 0.02~~ 0.65~~ 0.19]$ and then applied {\em transpose} to it using the $\top$ symbol in superscript. For vectors, such transposition simply converts them from column to row, and vice-versa. More on this operation appears below in the context of matrices.

For a data scientist and a machine learner, vectors are first and foremost considered as lists of numbers. But in fields such as physics, or applied mathematics, vectors also typically describe directions and magnitudes. This is very apparent in two dimensional spaces or three dimensional spaces. When we discuss gradient vectors in Section~\ref{sec:gradient} below, we also consider vectors as directions in space. But for now first let us think of vectors as lists of numbers, exactly playing the role of inputs, outputs, parameters, and intermediate computations in models such as our example models I -- III.

Note that single numbers, known as {\em scalars}, can be compared in the sense that one is greater than the other (as long as they are {\em real numbers} and not {\em complex numbers} which we do not often use in deep learning). For example we know that $2.4$ is greater than $-8.3$, denoted $-8.3 < 2.4$. If we treat all the scalar real numbers as the set ${\mathbb R}$, then we say there is an order on ${\mathbb R}$, because for every $u \in {\mathbb R}$ and $v \in {\mathbb R}$ one can determine if $u < v$ is true or false. With vectors, unlike scalars, there is not such an obvious order. 

While there is not an obvious universal way to order vectors, we can associate scalars (numbers) with the distances between two vectors, as well as with the length of individual vectors, and these scalars can then be used for ordering and other purposes. One of the most basic ways to do this is the {\em inner product} which for a vector $u \in {\mathbb R}^n$ and a vector $v \in {\mathbb R}^n$, is written as $u^\top v$ or sometimes as $u \cdot v$. The inner product is computed as,
%
\begin{equation}
\label{eq:inner product}
u^\top v = \sum_{i=1}^n u_i v_i = u_1 v_1 + u_2 v_2 + \ldots + u_n v_n.
\end{equation}
%
Hence it is a summation of the products of the individual entries of the vectors. So for example, as the reader may verify for $u=(2,\, 0, \, -3)$ and $v=(1,\, -12.5, \, 2)$, the inner product is $u^\top v = -4$. When the inner product is near $0$ it means that the vectors are quite different, whereas when the inner product is far from $0$ (either positive or negative) it means that the vectors are similar. When the inner product is exactly $0$ we say that the vectors are {\em orthogonal}. In a geometric representation this means the vectors are perpendicular. We do not dive into such a representation of vectors, and instead the interested reader can visit a text such as \cite{boyd2018introduction} for further reading.

If we now return to Model~I and revisit \eqref{eq:first-shallow-view} and \eqref{eq:smallz-only-log-mult}, then we can observe the inner product $w^\top x$ in these equations. Here $w$ is the vector of weight parameters of the model and $x$ is the model input.

One can also compute the {\em norm}  of a vector $u \in {\mathbb R}^n$, denoted as $\| u\|$ and computed as the square root of the inner product of the vector with itself (this is sometimes called the L2 norm or the Euclidean norm). That is,
%
\begin{equation}
\label{eq:norm-vec}
\| u \| = \sqrt{u^\top u} = \sqrt{\sum_{i=1}^n u_i^2}.
\end{equation}
%
For example for $u=(2, \, 0,\, -3)$ as before, $\| u \| = \sqrt{13} \approx 3.6$. The norm of a vector is always a non-negative number and is exactly $0$ if and only if all the entries of the vector are $0$, and otherwise it is strictly positive. So while we cannot order vectors in a unique manner, we can order vectors based on their norm which is a number  that summarizes their magnitude. 

Now that we have norm, we can also consider a scaled version of the inner product which is called the {\em cosine of the angle} between two vectors. For $u \in {\mathbb R}^n$ and $v \in {\mathbb R}^n$, as long as neither of these vectors is all $0$, this is computed as,
%
\begin{equation}
\label{eq:cosine-angle-vec}
\textrm{Cosine of the angle between $u$ and $v$} = \frac{u^\top v}{\|u \|\, \|v\|} = \frac{\sum_{i=1}^n u_i v_i}{\sqrt{\sum_{i=1}^n u_i^2} \, \sqrt{\sum_{i=1}^n v_i^2 }}.
\end{equation}
%
So for example for the $u$ and $v$ examples above, the cosine of the angle is about $-0.087$. The cosine of the angle is always between $-1$ and $1$ and the closer it is to $0$ the less similar the vectors are in some sense. It is exactly $0$ if the vectors are orthogonal.

All of the above definitions and computations of vectors are very common to use in deep learning. One more thing that we do is arithmetic with vectors. The most basic operation is to take a scalar (a single number) and multiply each element of the vector by this scalar. This is called {\em scalar multiplication} (or sometimes {\em scalar-vector multiplication}). So for a scalar, $\alpha \in {\mathbb R}$ and a vector $u \in {\mathbb R}^n$, the scalar multiplication $\alpha \, u$ is a new vector where at coordinate~$i$ it has $\alpha \, u_i$. As an example, let us return to $u=(2,\,0,\,-3)$ from above, and say that $\alpha = -4$, we have that $\alpha \, u = (-8, \,0,\, 12)$.

Let us connect this to the softmax function. If we consider the right side of \eqref{eq:softeq} then we can observe that this is in fact a case of scalar multiplication. In particular the expression $1/\sum_{i=1}^K e^{z_i}$ is a scalar which multiplies the vector $(e^{z_1},\ldots,e^{z_K})$. In this case, given some input $z \in {\mathbb R}^K$, the softmax function transforms $z$ such that all entries are positive via the exponentiation. It also ensures the sum of the entries is $1$ via the scalar multiplication. Importantly, the result of the softmax transforms an arbitrary vector of numbers to a vector of probabilities, maintaining the same order. For example if as input to the softmax function we have $z = (0.03,\, -1.91, \, 1.56, \, 0.34)$, then it is already evident that the third entry $z_3 = 1.56$ is the maximal, but this is not quantified in terms of probabilities. Then after exponentiation we have $(e^{z_1}, e^{z_2}, e^{z_3}, e^{z_4}) = (1.03,\,0.148,\,4.807,\,1.405)$. Now we can compute that $1/\sum_{i=1}^K e^{z_i} = 0.1353$, and then by applying scalar multiplication of this value by $(e^{z_1}, e^{z_2}, e^{z_3}, e^{z_4})$, we arrive at $\hat{y}$ as in \eqref{eq:example-y-hat} (note that this is approximate due to rounding).

In addition to scalar multiplication we can also add two vectors of the same dimension. This works by adding the individual matching coordinates. So for $u \in {\mathbb R}^n$ and $v \in {\mathbb R}^n$, the addition $u+v$ is a new vector, where at coordinate $i$ it has $u_i + v_i$. So again with $u$ and $v$ as in the example above, we have that, $u+v = (3, \, -12.5, \, -1)$. Now subtraction can also be defined by scalar-multiplying the second vector by $-1$ and then adding. So for example $u-v = (1,\, 12.5,\, -5)$. Note that if we subtract two vectors that are equal then we get a vector of all $0$ values, called the {\em zero vector}.

Having vector subtraction allows us to use the vector norm to define the distance between two vectors. In particular, given a vector $u \in {\mathbb R}^n$ and a vector $v \in {\mathbb R}^n$, the distance (also known as Euclidean distance) between the two vectors is $\|u-v\|$. Hence we first subtract the vectors and then compute the norm of the answer. That is,
%
\begin{equation}
\label{eq:euc-distance}
\textrm{Distance between vectors $u$ and $v$} =
\| u - v\| = \sqrt{\sum_{i=1}^n (u_i - v_i)^2}.
\end{equation}
%
This number is never negative, and the closer it is to $0$ the closer that $u$ is to $v$. For the example vectors in ${\mathbb R}^3$ above, we have that $\|u-v\| = 13.5$. Note that sometimes we consider a similar quantity without the square root. This is naturally denoted as $\|u-v\|^2$, and it can sometimes be called the {\em squared error} between the vectors. It can also be represented in terms of the innner product of the difference $u-v$ and itself,
%
\begin{equation}
\label{eq:euc-mse-l2}
\textrm{Squared error between $u$ and $v$} =
\| u - v\|^2 = (u-v)^\top(u-v) = \sum_{i=1}^n (u_i - v_i)^2.
\end{equation}
%
It turns out that variants of the squared error as in \eqref{eq:euc-mse-l2} naturally play a role as loss functions in deep learning (as well as classical statistics). In particular, one of the vectors, say $u$ can play the role of desired model output, often denoted $y$, whereas the other vector, $v$, is the predicted model output, $\hat{y}$. In this case, $\|y - \hat{y}\|^2$ is a measure of how far the obtained output $\hat{y}$ is from what it should have been. 

To make this more concrete, say we are using Model~II for multi-class classification with $K=4$ classes. After the model is trained, one can then consider a test set of say $1000$ samples of inputs $x^{(1)},\ldots,x^{(1000)}$ (each of these is a vector of some dimension $p$, e.g., $p=300$). We then apply the model on each of these vectors and get $1000$ result vectors which we denote as  $\hat{y}^{(1)}, \ldots, \hat{y}^{(1000)}$. Each of these results vectors look like the vector in \eqref{eq:example-y-hat} only generally has different probabilities.

We now wish to compare the result vectors to what they would ideally be. For this, assume that we also have outcomes which indicate for each observation if it is the first class, second class, third class, or fourth class. One thing we can do with this is to create a set of vectors called {\em one-hot encoded vectors}. If an observation is in the first class, the associated one-hot encoded vector is $(1,0,0,0)$. If it is in the second class, the one-hot encoded vector is $(0,1,0,0$). And so fourth. These vectors are also called the {\em canonical unit vectors}. Observe also that they represent probability vectors, with probabilities being degenerative in the sense that all the probability mass is at one position while other positions have $0$ probability. With this we create the $1000$ one-hot encoded vectors $y^{(1)}, \ldots, y^{(1000)}$. We then define loss or error as
%
\begin{equation}
 \label{eq:mse-error-class-1000}   
\text{Mean squared error between $y$ and $\hat{y}$} = \frac{1}{1000}\sum_{i=1}^{1000} \| y^{(i)} - \hat{y}^{(i)} \|^2.
\end{equation}
%
Here we are averaging the squared errors between each desired (one-hot encoded) $y^{(i)}$ and obtained prediction $\hat{y}^{(i)}$. If our model is perfect then this mean squared error will be $0$, but generally it is positive, yet the lower it is, the better our predictions.

We note that in practice, for Model~I and Model~II we typically use a cross entropy loss which differs from this simpler mean squared error. For details, see for example Chapter~3 of  \cite{LiquetMokaNazarathy2024DeepLearning}.
