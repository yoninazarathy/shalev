\relax 
\citation{lecun2015deep}
\citation{howard2020deep}
\citation{LiquetMokaNazarathy2024DeepLearning}
\citation{prince2023understanding}
\citation{goodfellow2016deep}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Some Motivating Deep Learning Models}{3}{}\protected@file@percent }
\newlabel{sec:motivating-deep-learning-models}{{2}{3}}
\newlabel{eq:first-shallow-view}{{1}{4}}
\newlabel{eq:smallz-only-log-mult}{{2}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The sigmoid model represented with neural network terminology as a shallow neural network. Observe that the artificial neuron is composed of an affine transformation using $z = b_0 + w^\top x$ followed by a non-linear activation transformation $\sigma _{\text  {Sig}}(z)$.\relax }}{4}{}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:niceneuron}{{1}{4}}
\newlabel{eq:y-hat-tau}{{3}{5}}
\newlabel{eq:softeq}{{4}{5}}
\newlabel{eq:small-zk-log-mult}{{5}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The softmax model as a shallow neural network model with output $\hat  {y}$ which is composed of elements $\hat  {y}_1, \ldots  , \hat  {y}_K$, representing a probability vector. \relax }}{6}{}\protected@file@percent }
\newlabel{fig:shalsoft}{{2}{6}}
\newlabel{eq:argmax-mp}{{6}{6}}
\newlabel{eq:generalRecursiveModel}{{7}{7}}
\newlabel{eq:dense-layer}{{8}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Data Standardization and Recalling Summation Notation}{7}{}\protected@file@percent }
\newlabel{sec:summation}{{3}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A fully connected feedforward deep neural network with multiple hidden layers. The input to the network is the vector $x = (x_1, x_2, x_3, x_4)$ and in this case the output is the scalar\nobreakspace  {}$\hat  {y}$. For this particular example the dimensions of $W^{[1]}$ are $4\times 4$, $W^{[2]}$ is $3 \times 4$, $W^{[3]}$ is $5 \times 3$, and $W^{[4]}$ is $1 \times 5$. The bias vectors are of dimension $4$ for $b^{[1]}$, $3$ for $b^{[2]}$, $5$ for $b^{[3]}$, and $1$ (a scalar) for $b^{[4]}$. \relax }}{8}{}\protected@file@percent }
\newlabel{FFNN}{{3}{8}}
\newlabel{eq:data-table}{{9}{8}}
\newlabel{eq:stats-mean-var}{{10}{9}}
\newlabel{eq:4-summation}{{11}{9}}
\newlabel{eq:4-summation-advc}{{12}{9}}
\newlabel{eq:ref-stand-z}{{13}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {4}From Basic Sets to Functions}{10}{}\protected@file@percent }
\newlabel{sec:sets-and-functions}{{4}{10}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Vectors}{10}{}\protected@file@percent }
\newlabel{sec:vectors}{{5}{10}}
\newlabel{eq:example-y-hat}{{14}{10}}
\citation{boyd2018introduction}
\newlabel{eq:inner product}{{15}{11}}
\newlabel{eq:norm-vec}{{16}{11}}
\newlabel{eq:cosine-angle-vec}{{17}{11}}
\newlabel{eq:euc-distance}{{18}{12}}
\newlabel{eq:euc-mse-l2}{{19}{12}}
\citation{LiquetMokaNazarathy2024DeepLearning}
\citation{boyd2018introduction}
\newlabel{eq:mse-error-class-1000}{{20}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Matrices}{13}{}\protected@file@percent }
\newlabel{sec:matrices}{{6}{13}}
\newlabel{eq:matrix-example-1}{{21}{13}}
\citation{nazarathy2020statistics}
\newlabel{eq:identity}{{22}{14}}
\newlabel{eq:vec-as-matrix}{{23}{14}}
\newlabel{eq:matrix-example-1-t}{{24}{14}}
\newlabel{eq:matrix-a-b-for-mult}{{25}{14}}
\newlabel{eq:matrix-example}{{26}{15}}
\newlabel{eq:matrix-example-2}{{27}{15}}
\citation{boyd2018introduction}
\citation{strang2019linear}
\citation{LiquetMokaNazarathy2024DeepLearning}
\newlabel{eq:matrix-vector-mult}{{28}{16}}
\newlabel{eq:model-ii-vector-add}{{29}{16}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Further Notes About Vectors and Matrices}{16}{}\protected@file@percent }
\newlabel{sec:further-notes}{{7}{16}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Gradients}{17}{}\protected@file@percent }
\newlabel{sec:gradient}{{8}{17}}
\citation{wilkes2016burn}
\newlabel{eq:grad-def}{{30}{18}}
\citation{LiquetMokaNazarathy2024DeepLearning}
\citation{kingma2014adam}
\citation{LiquetMokaNazarathy2024DeepLearning}
\newlabel{eq:gdls1}{{31}{19}}
\@writefile{loa}{\contentsline {algocf}{\numberline {}{\ignorespaces Gradient descent with loss $C(\theta )$\relax }}{19}{}\protected@file@percent }
\newlabel{alg:basic-gradient-descent}{{}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Illustration of gradient descent for 5 iterations starting with $\theta ^{(0)}$ and getting near the optimum $\theta ^*$ with $\theta ^{(5)}$. (a)\nobreakspace  {}A one-dimensional loss landscape $C(\theta )$ with $\theta \in \mathbb  {R}$. (b)\nobreakspace  {}a two-dimensional loss landscape $C(\theta _1,\theta _2)$ with $\theta = (\theta _1, \theta _2) \in \mathbb  {R}^2$.\relax }}{20}{}\protected@file@percent }
\newlabel{simpleloss}{{4}{20}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Putting Bits Together Into a Deep Neural Network}{20}{}\protected@file@percent }
\newlabel{sec:putting-bits-together}{{9}{20}}
\citation{LiquetMokaNazarathy2024DeepLearning}
\citation{LiquetMokaNazarathy2024DeepLearning}
\newlabel{eq:mod3-approx}{{32}{21}}
\newlabel{eq:s-deep-step}{{33}{21}}
\newlabel{eq:nn-example-dims}{{34}{21}}
\newlabel{eqn:opened-out-example-2}{{35}{21}}
\citation{LiquetMokaNazarathy2024DeepLearning}
\newlabel{eq:61-is-the-number}{{36}{22}}
\@writefile{toc}{\contentsline {section}{\numberline {10}More Advanced Models}{22}{}\protected@file@percent }
\newlabel{sec:more-advanced-models}{{10}{22}}
\newlabel{eq:conv-kernel}{{37}{22}}
\newlabel{eq:conv-explicit}{{38}{22}}
\citation{LiquetMokaNazarathy2024DeepLearning}
\citation{krizhevsky2012imagenetxxxqqq}
\citation{LiquetMokaNazarathy2024DeepLearning}
\citation{vaswani2017attention}
\citation{LiquetMokaNazarathy2024DeepLearning}
\newlabel{eq:very-spcific-conv}{{39}{23}}
\citation{LiquetMokaNazarathy2024DeepLearning}
\newlabel{eq:score}{{40}{24}}
\newlabel{eq:attentionweight}{{41}{24}}
\citation{LiquetMokaNazarathy2024DeepLearning}
\bibstyle{apalike}
\bibdata{BibForTheDLBook.bib}
\@writefile{toc}{\contentsline {section}{\numberline {11}Conclusion and Outlook}{25}{}\protected@file@percent }
\newlabel{sec:conclusion}{{11}{25}}
\gdef \@abspage@last{25}
